{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# for LR\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import KFold\n",
    "import random\n",
    "\n",
    "# for logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# for Evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# for feature eng\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data = pd.read_csv('/Users/josephcolaco/customer_revenue_prediction/data/train.csv')\n",
    "test_data = pd.read_csv('/Users/josephcolaco/customer_revenue_prediction/data/test.csv')\n",
    "print('Shape of test data is', test_data.shape)\n",
    "print('Shape of train data is',train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do for data cleaning\n",
    "- json to dataframe columns\n",
    "- format various data types\n",
    "- need to format the date field\n",
    "- visitNumber as int\n",
    "- format visitStartTime\n",
    "- format visits\n",
    "- format campaignCode\n",
    "- deal with nulls\n",
    "- drop columns that are not needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert json columns to df format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def json_to_lst(model_data, json_cols):\n",
    "    for column in json_cols:\n",
    "        model_data = model_data.join(pd.DataFrame(\n",
    "            model_data.pop(column).apply(pd.io.json.loads).values.tolist(), index=model_data.index)) \n",
    "    \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "train_data_v1 = json_to_lst(train_data, json_cols)\n",
    "test_data_v1 = json_to_lst(test_data, json_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'adwordsClickInfo' feild is a dictionary. Let's change that to a dataframe columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "concat_df = pd.DataFrame(train_data_v1['adwordsClickInfo'].tolist())\n",
    "train_data_v2 = train_data_v1.drop(columns = ['adwordsClickInfo'])\n",
    "train_data_v3 = pd.concat([train_data_v2,concat_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Columns with No Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of train data version 3 is:', train_data_v3.shape)\n",
    "train_data_v3.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is not available in several columns in the dataset. These columns will be dropped from the dataframe. 'visits' field will also be deleted as it has only one value in the column ('1')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4 = train_data_v3.drop(columns = ['browserSize', 'browserVersion', 'flashVersion', 'language','mobileDeviceBranding', \n",
    "                             'mobileDeviceInfo', 'mobileDeviceMarketingName', 'mobileDeviceModel', \n",
    "                              'mobileInputSelector', 'operatingSystemVersion', 'screenColors', 'screenResolution',\n",
    "                             'cityId', 'latitude', 'longitude', 'networkLocation', 'targetingCriteria', \n",
    "                                              'criteriaParameters', 'visits', 'socialEngagementType', 'isTrueDirect'])\n",
    "\n",
    "print('Shape of train_data version 4 is:', train_data_v4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proper Formating for Data Fields:\n",
    "- date to datetime\n",
    "- hits to int\n",
    "- newVisits to int\n",
    "- pageviews to int\n",
    "- transactionRevenue to float64\n",
    "- page to int\n",
    "- bounces to int\n",
    "- visitStartime to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4['date'] = pd.to_datetime(train_data_v4['date'], format='%Y%m%d')\n",
    "train_data_v4['transactionRevenue'] = train_data_v4['transactionRevenue'].fillna(1).astype('float64')\n",
    "train_data_v4['hits'] = train_data_v4['hits'].fillna(0).astype('int64')\n",
    "train_data_v4['newVisits'] = train_data_v4['newVisits'].fillna(0).astype('int64')\n",
    "train_data_v4['pageviews'] = train_data_v4['pageviews'].fillna(0).astype('int64')\n",
    "train_data_v4['page'] = train_data_v4['page'].fillna(0).astype('int64')\n",
    "train_data_v4['bounces'] = train_data_v4['bounces'].fillna(0).astype('int64')\n",
    "train_data_v4['isVideoAd'] = train_data_v4['isVideoAd'].fillna('False').astype('bool')\n",
    "train_data_v4['visitStartTime'] = train_data_v4['visitStartTime'].apply(lambda x: dt.datetime.utcfromtimestamp(float(x)))\n",
    "train_data_v4['visitHour'] = train_data_v4['visitStartTime'].dt.hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_df = pd.DataFrame(train_data_v4.isnull().sum(), columns = ['count_null_rows'])\n",
    "# null_df = null_df[null_df['count_null_rows']>0]\n",
    "null_df = null_df[null_df['count_null_rows']>0].sort_values(by='count_null_rows', ascending=False)\n",
    "null_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill all object column nans with '(not set)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_data_v4.columns:\n",
    "    if train_data_v4[i].isnull().sum() > 0:\n",
    "        train_data_v4[i] = train_data_v4[i].fillna('(not set)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all 'not available in demo dataset' to '(not set)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4 = train_data_v4.replace(['not available in demo dataset', '(not set)'], ['(not_set)', '(not_set)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transaction Revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4 = train_data_v4.assign(\n",
    "    logtransactionRevenue = np.log(train_data_v4.transactionRevenue))\n",
    "train_data_v4 = train_data_v4.drop(columns=['transactionRevenue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Binary Made Purchase Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4['madePurchase'] = np.where(train_data_v4['logtransactionRevenue'] > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Unique Identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4.head()\n",
    "print('shape of train data version 4:', train_data_v4.shape[0])\n",
    "print('number of unique session_ids is:', train_data_v4.sessionId.nunique())\n",
    "print('diff is:', train_data_v4.shape[0]-train_data_v4.sessionId.nunique())\n",
    "print('The sessionId should be a unique identifier. Investigate duplicates:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max count of duplicate sessionId is 2. This method does the following: \n",
    "    - if train data\n",
    "        - Delete first of two duplicate rows as long as the first row does not have a transaction revenue > 0\n",
    "    - if test_data\n",
    "        - delete the first row of duplicate sessionId\n",
    "Only 1 row in the train_data has a transacton revenue > 0 removed and thats because bith duplicate sessions had a revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_session_id_dup(data, is_train):\n",
    "    sessionId_counts = pd.DataFrame(data['sessionId'].value_counts()).reset_index()\n",
    "    dup_sessionId = sessionId_counts['index'][sessionId_counts['sessionId']>1].values.tolist()\n",
    "    dup_data = data[data['sessionId'].isin(dup_sessionId)].sort_values(by='sessionId')\n",
    "    \n",
    "    index_to_delete = []\n",
    "    count = 0\n",
    "    delete = False\n",
    "    for index, row in dup_data.iterrows():\n",
    "        if is_train:\n",
    "            if count == 0 and row['madePurchase'] == 1: #leaves the first duplicate if the person has made a purchase\n",
    "                count +=1\n",
    "                delete = False\n",
    "                continue\n",
    "            elif count == 0 and delete == False: #deletes the first duplicate if person hasnt made a purchase\n",
    "                index_to_delete.append(index)\n",
    "                delete = True\n",
    "                count +=1\n",
    "            elif count == 1 and delete == False: #deletes the second duplicate if person has made a purchas in the first row\n",
    "                index_to_delete.append(index)\n",
    "                count = 0\n",
    "            elif count == 1 and delete == True: #assign delete to false and continues\n",
    "                delete = False\n",
    "                count = 0\n",
    "                continue\n",
    "        elif count == 0 and delete == False:\n",
    "            index_to_delete.append(index)\n",
    "            delete = True\n",
    "            count +=1\n",
    "        else:\n",
    "            delete = False\n",
    "            count = 0\n",
    "            \n",
    "    data = data.reset_index()\n",
    "    print('Deleted', len(index_to_delete), 'rows!')\n",
    "    print('Shape of data before deleting duplicates:', data.shape)\n",
    "    data_v1 = data[~data['index'].isin(index_to_delete)]\n",
    "    print('Shape of data after deleting duplicates:', data_v1.shape)   \n",
    "\n",
    "    return index_to_delete, data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_index_delete, train_data_v5 = remove_session_id_dup(train_data_v4, True)\n",
    "test_index_delete, test_data_v1 = remove_session_id_dup(test_data, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Day, Month and Year Columns from Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v5['dayNameDate'] = train_data_v5['date'].dt.day_name()\n",
    "train_data_v5['monthDate'] = train_data_v5['date'].dt.month\n",
    "train_data_v5['yearDate'] = train_data_v5['date'].dt.year\n",
    "train_data_v5['dayDate'] = train_data_v5['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_v5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(x, bins, xlabel, ylabel, title):\n",
    "#     plt.style.use('ggplot')\n",
    "    plt.style.use('seaborn')\n",
    "    plt.hist(x, bins)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(x, y,xlabel, ylabel, title):\n",
    "    plt.style.use('seaborn')\n",
    "    plt.scatter(x, y, alpha=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sns_scatter(data, x, y, hue):\n",
    "    sns.lmplot(x=x, y=y, data=data, \n",
    "               fit_reg=False, legend=False, hue = hue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cat_bar(data, col, title):\n",
    "    plt.style.use('seaborn')\n",
    "    col_value_counts = pd.DataFrame(data[col].value_counts())\n",
    "    if col_value_counts.shape[0] > 10:\n",
    "        col_value_counts = col_value_counts[:10]\n",
    "    plt.bar(col_value_counts.index, col_value_counts[col])\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis on Record Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis on how many records made a purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_value_counts = pd.DataFrame(train_data_v5['madePurchase'].value_counts())\n",
    "print('Percentage of records that made a transaction:', \n",
    "      round(train_data_v5[train_data_v5['madePurchase'] ==1].shape[0]/train_data_v5.shape[0], 2))\n",
    "col_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=train_data_v5['madePurchase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v5.groupby('madePurchase').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can tell clearly from above that the records that made a purchase are ones that have high pageviews, hits, 0 bounces and often not from a mobile device. The transactions are generally later in the day as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v5.groupby('deviceCategory').mean()\n",
    "# a major proportion of the logtransactionrevenue is from desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_v4.groupby('channelGrouping').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v4.groupby('continent').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.deviceCategory, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.operatingSystem, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.isMobile, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.browser, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.continent, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.metro, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.monthDate, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cross_tab_df = pd.crosstab(train_data_v5.dayDate, train_data_v5.madePurchase)\n",
    "cross_tab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_value_counts = pd.DataFrame(train_data_v5['networkDomain'].value_counts())\n",
    "# print('Percentage of records that made a transaction:', \n",
    "#       round(train_data_v5[train_data_v5['madePurchase'] ==1].shape[0]/train_data_v5.shape[0], 2))\n",
    "col_value_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "object_cols = train_data_v4.select_dtypes(include='object').columns\n",
    "for i in object_cols:\n",
    "    if i != 'fullVisitorId' and i != 'sessionId':\n",
    "        if train_data_v4[i].nunique() <= 50:\n",
    "            print(i)\n",
    "            plot_cat_bar(train_data_v4, i, str(i) + ' vs frequency')\n",
    "# plt.bar(test.index, test.channelGrouping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(train_data_v5['logtransactionRevenue'][train_data_v5['logtransactionRevenue'] > 0], 10, \n",
    "          'Log TransactionRevenue', 'Frequency', 'Hist of Log TransactionRevenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train_data_v4.select_dtypes(include='number').columns\n",
    "for i in numeric_cols:\n",
    "    if i != 'visitId':\n",
    "        plot_hist(train_data_v4[i], 10, \n",
    "          str(i), 'Frequency', 'Hist of ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas for Modeling\n",
    "1) Delete all records that contain a web browser with not purchases. Train a model to classify madepurchase and then train another model to predict the logtransaction revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(train_data_v4['visitNumber'][train_data_v4['visitNumber'] < 50], 10, \n",
    "          'visitNumber', 'Frequency', 'Hist of visitNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(train_data_v4['hits'][train_data_v4['hits'] < 50], 10, \n",
    "          'hits', 'Frequency', 'Hist of hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hist(train_data_v4['pageviews'][train_data_v4['pageviews'] < 50], 10, \n",
    "          'pageviews', 'Frequency', 'Hist of pageviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = train_data_v4.select_dtypes(include='number')\n",
    "corr_LTR = pd.DataFrame(\n",
    "    numeric_data.corrwith(numeric_data['logtransactionRevenue'], axis=0, drop=False), columns = ['corr'])\n",
    "corr_LTR.sort_values(by='corr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in numeric_cols:\n",
    "    if i != 'visitId':\n",
    "        plot_sns_scatter(train_data_v4, i, 'logtransactionRevenue', 'madePurchase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering on Columns with High Number of Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_unique = pd.DataFrame(train_data_v5.nunique(), columns=['count_unique'])\n",
    "n_unique = n_unique.sort_values(by='count_unique', ascending=False)\n",
    "print('These are all the columns and the number of unique values in them:')\n",
    "n_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on networkDomain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v5['networkDomain'] = train_data_v5['networkDomain'].replace(['unknown.unknown'], ['(not_set)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkDomain = train_data_v5[['networkDomain','madePurchase']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkDomain['networkDomain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "networkDomain['splitNetworkDomain'] = networkDomain['networkDomain'].str.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkDomain['topLevelDomain'] = networkDomain['networkDomain'].str.split('.').str[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are now', networkDomain['topLevelDomain'].nunique(), 'unique values for top level domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Here are the value counts:')\n",
    "networkDomain['topLevelDomain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_crosstab = pd.crosstab(networkDomain.topLevelDomain, networkDomain.madePurchase)\n",
    "network_crosstab.sort_values(network_crosstab.columns[1], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_network_domain(data):\n",
    "    top_level_domain = ['(not_set)', '.us', '.net', '.com', '.edu', '.ca', '.org', '.mx' ]\n",
    "    data_v1 = data\n",
    "    for i in top_level_domain:\n",
    "        col_name = 'tl_' + str(i)\n",
    "        data_v1[col_name] = data_v1['networkDomain'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "networkDomain_v1 = pop_network_domain(networkDomain)\n",
    "networkDomain_v1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6 = pop_network_domain(train_data_v5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on gclID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6['gclId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v6.gclId, train_data_v6.madePurchase)\n",
    "crosstab.sort_values(crosstab.columns[1], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of my lack of understanding of gclId I will not use this in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing on keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6['keyword'] = train_data_v6['keyword'].replace(['(not provided)'], ['(not_set)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6['keyword'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v6.keyword, train_data_v6.madePurchase)\n",
    "crosstab.sort_values(crosstab.columns[1], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    sentence = re.sub('[^a-zA-Z]+', ' ', sentence)\n",
    "    tokens = sentence.split()\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words] \n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens if len(token) > 1]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "porter_stemmer = PorterStemmer()\n",
    "train_data_v6['cleanKeyword_v2'] = train_data_v6['keyword'].apply(stem_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v6['cleanKeyword'].nunique()\n",
    "train_data_v6['cleanKeyword_v2'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v6.cleanKeyword_v2, train_data_v6.madePurchase)\n",
    "crosstab.sort_values(crosstab.columns[1], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w_purchase = crosstab.sort_values(crosstab.columns[1], ascending = False)[1:7].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_w_purchase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = []\n",
    "for i in keyword_w_purchase:\n",
    "    for j in i.split(' '):\n",
    "        if len(j) > 2:\n",
    "            lst_words.append(j)\n",
    "lst_words      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words_df = pd.DataFrame(lst_words, columns=['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words_df_v2 = pd.DataFrame(lst_words_df['words'].unique(), columns=['words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lst_words_df.shape)\n",
    "print(lst_words_df_v2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words_final = lst_words_df_v2['words'].values\n",
    "lst_words_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_keyword(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words_final = ['qehscssdk', 'googl', 'merchandis', 'store', 'hzbaqlcbjwfgoh', \n",
    "                       'remarket', 'content', 'target', 'zknv']\n",
    "    data_v1 = data\n",
    "    data_v1['clean_keyword'] = data_v1['keyword'].apply(stem_sentences)\n",
    "    for i in lst_words_final:\n",
    "        col_name = 'keyword_' + str(i)\n",
    "        data_v1[col_name] = data_v1['clean_keyword'].str.contains(i, case=1, na=0)\n",
    "        \n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_v7 = transform_keyword(train_data_v6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on referralPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v7['referralPath'] = train_data_v7['referralPath'].replace(['/'], ['(not_set)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_data_v7['referralPath'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v6.referralPath, train_data_v6.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = crosstab[1]/crosstab[0]\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_crosstab = crosstab[(crosstab[1] > 5) & (crosstab[2] > 0.01) & (crosstab.index != \"(not_set)\")]\n",
    "filterd_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_crosstab_index = pd.DataFrame(filterd_crosstab.index)\n",
    "filterd_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'hela  ssd  '\n",
    "a.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences_v2(sentence):\n",
    "    sentence = re.sub('[^a-zA-Z]+', ' ', sentence)\n",
    "    tokens = sentence.split('/')\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words] \n",
    "    stemmed_tokens = [porter_stemmer.stem(token.strip()) for token in tokens if len(token) > 1]\n",
    "    return ''.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_crosstab_index['split'] = filterd_crosstab_index['referralPath'].apply(stem_sentences_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_lst = filterd_crosstab_index['split'].values\n",
    "split_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_split_words = []\n",
    "for i in split_lst:\n",
    "    for j in i.split(' '):\n",
    "        if len(j) > 2:\n",
    "            lst_split_words.append(j)\n",
    "lst_split_words_df = pd.DataFrame(lst_split_words, columns=['words'])\n",
    "lst_split_words_df_unique = lst_split_words_df['words'].unique()\n",
    "lst_split_words_df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_referralPath(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words_final = ['deal', 'sign', 'google', 'merchandise', 'store', 'emails',\n",
    "                       'special', 'coup', 'stor', 'mail', 'com', 'forum', 'merch', 'new',\n",
    "                       'url', 'site', 'mountain', 'view', 'php', 'offer', 'googletopia', \n",
    "                       'free', 'stuff', 'alphabet','discount']\n",
    "    data_v1 = data\n",
    "    data_v1['clean_referralPath'] = data_v1['referralPath'].apply(stem_sentences_v2)\n",
    "    for i in lst_words_final:\n",
    "        col_name = 'referralPath_' + str(i)\n",
    "        data_v1[col_name] = data_v1['clean_referralPath'].str.contains(i, case=1, na=0)\n",
    "        \n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_v8 = transform_referralPath(train_data_v7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(train_data_v8['source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.source, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_crosstab = crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filterd_crosstab_index = filterd_crosstab.index\n",
    "filterd_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterd_crosstab_index = filterd_crosstab.index\n",
    "filterd_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_source(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['mall.googleplex.com', 'dealspotr.com', 'mail.google.com',\n",
    "       'groups.google.com', 'phandroid.com', 'gdeals.googleplex.com', 'dfa',\n",
    "       'l.facebook.com', 'yahoo', 'google', 'bing', 'sites.google.com',\n",
    "       '(direct)', 'facebook.com']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'source_' + str(i)\n",
    "        data_v1[col_name] = data_v1['source'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_data_v9 = transform_source(train_data_v8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v9.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on region"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_v9['region'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.region, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab[:34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = crosstab[:34].index\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_region(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Zulia', 'Nebraska', 'Michigan', 'Pichincha', 'Tennessee', 'Illinois',\n",
    "       'New York', 'Washington', 'Massachusetts', 'Colorado', 'South Carolina',\n",
    "       'Texas', 'Georgia', 'Missouri', 'Iowa', 'District of Columbia',\n",
    "       'California', 'Minnesota', 'Utah', 'Arizona', 'Pennsylvania',\n",
    "       'New Jersey', 'Indiana', 'Florida', 'Maryland', 'Connecticut',\n",
    "       'North Carolina', 'Virginia', 'Nevada', 'Ohio', 'Alberta', '(not_set)',\n",
    "       'Ontario', 'Oregon']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'source_' + str(i)\n",
    "        data_v1[col_name] = data_v1['source'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v10 = transform_region(train_data_v9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on channelGrouping and medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v10['channelGrouping'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v10['medium'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_data_v10[['medium', 'channelGrouping']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"med_channel\"] = test[\"medium\"].map(str) + '_' + test[\"channelGrouping\"].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['med_channel'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to delete the medium column and just keep channel grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v11 = train_data_v10.drop(columns = ['medium'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on operatingSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v11['operatingSystem'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.operatingSystem, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_operatingSystem(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Chrome OS', 'Macintosh', 'Linux', 'iOS', 'Windows']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'operatingSystem_' + str(i)\n",
    "        data_v1[col_name] = data_v1['operatingSystem'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v12 = transform_operatingSystem(train_data_v11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on adContent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_v11['adContent'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.adContent, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_adContent(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Google Merchandise Collection', '(not_set)']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'adContent' + str(i)\n",
    "        data_v1[col_name] = data_v1['adContent'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v13 = transform_adContent(train_data_v12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v13['browser'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.browser, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_browser(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Chrome', 'Firefox', 'Internet Explorer', 'Edge']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'browser' + str(i)\n",
    "        data_v1[col_name] = data_v1['browser'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v14 = transform_browser(train_data_v13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v14.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v14['metro'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.metro, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decided to use region instead of metro since it is cleaner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v15= train_data_v14.drop(columns=['metro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.country, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_country(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['United States', 'Venezuela', 'Puerto Rico', 'Canada']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'country' + str(i)\n",
    "        data_v1[col_name] = data_v1['country'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v16 = transform_country(train_data_v15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.campaign, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_campaign(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['AW - Dynamic Search Ads Whole Site', 'AW - Accessories', '(not_set)']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'campaign' + str(i)\n",
    "        data_v1[col_name] = data_v1['campaign'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v17 = transform_campaign(train_data_v16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on subContinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.subContinent, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decided to delete the continent column and just keep one or two one hotebcoding columns from the subcontinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_subContinent(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Northern America', 'Caribbean']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'subContinent' + str(i)\n",
    "        data_v1[col_name] = data_v1['subContinent'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v18 = transform_subContinent(train_data_v17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focusing in on city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v15['city'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(train_data_v8.city, train_data_v8.madePurchase)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[1], ascending = False)\n",
    "crosstab[2] = round(crosstab[1]/crosstab[0],2)\n",
    "crosstab = crosstab.sort_values(crosstab.columns[2], ascending = False)\n",
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_crosstab =  crosstab[(crosstab[1] > 5) & (crosstab[2] >= 0.01)]\n",
    "filtered_crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_crosstab_index = filtered_crosstab.index.values\n",
    "filtered_crosstab_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_city(data):\n",
    "    # given the data, want to make binary columns stating that they contain necessary keywords \n",
    "    lst_words = ['Maracaibo', 'Ann Arbor', 'Cambridge', 'San Bruno', 'Chicago',\n",
    "       'Austin', 'Irvine', 'New York', 'Nashville', 'Jersey City',\n",
    "       'Boulder', 'Kirkland', 'Seattle', 'Oakland', 'Denver', 'Sunnyvale',\n",
    "       'San Francisco', 'Pittsburgh', 'Washington', 'Atlanta',\n",
    "       'Los Angeles', 'Mountain View', 'Minneapolis', 'San Antonio',\n",
    "       'Lake Oswego', 'Santa Clara', 'Cupertino', 'Salem', 'San Mateo',\n",
    "       'San Diego', 'Palo Alto', 'Fremont', 'Houston', 'Milpitas',\n",
    "       'Boston', 'Charlotte', 'San Jose', 'Philadelphia', 'Redwood City',\n",
    "       'Portland', 'Phoenix', '(not_set)', 'Toronto', 'Dallas']\n",
    "    data_v1 = data\n",
    "    for i in lst_words:\n",
    "        col_name = 'city' + str(i)\n",
    "        data_v1[col_name] = data_v1['city'].str.contains(i, case=1, na=0)\n",
    "    return data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v19 = transform_city(train_data_v18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v19.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods to Do all Data Cleaning and Feature Engineering For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of test data is (804684, 12)\n",
      "Shape of train data is (903653, 12)\n",
      "CPU times: user 19.9 s, sys: 1.68 s, total: 21.6 s\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data = pd.read_csv('/Users/josephcolaco/customer_revenue_prediction/data/train.csv', \n",
    "                        dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\n",
    "test_data = pd.read_csv('/Users/josephcolaco/customer_revenue_prediction/data/test.csv', \n",
    "                       dtype={'date': str, 'fullVisitorId': str, 'sessionId':str}, nrows=None)\n",
    "print('Shape of test data is', test_data.shape)\n",
    "print('Shape of train data is',train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_lst(model_data, json_cols):\n",
    "    for column in json_cols:\n",
    "        model_data = model_data.join(pd.DataFrame(\n",
    "            model_data.pop(column).apply(pd.io.json.loads).values.tolist(), index=model_data.index)) \n",
    "    \n",
    "    return model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(data, is_train):\n",
    "    # json columns to list\n",
    "    json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']\n",
    "    data = json_to_lst(data, json_cols)\n",
    "    # adwordsClickInfo still in json column\n",
    "    concat_df = pd.DataFrame(data['adwordsClickInfo'].tolist())\n",
    "    data = pd.concat([data,concat_df], axis=1)\n",
    "    \n",
    "    # dropping columns that have null values or provide no information\n",
    "    data = data.drop(columns = ['adwordsClickInfo', 'browserSize', 'browserVersion', 'flashVersion', 'language',\n",
    "                                'mobileDeviceBranding', 'mobileDeviceInfo', 'mobileDeviceMarketingName', \n",
    "                                'mobileDeviceModel', 'mobileInputSelector', 'operatingSystemVersion', 'screenColors',\n",
    "                                'screenResolution', 'cityId', 'latitude', 'longitude', 'networkLocation', \n",
    "                                'targetingCriteria','criteriaParameters', 'visits', 'socialEngagementType', \n",
    "                                'isTrueDirect'])\n",
    "    \n",
    "    # adjusting data types\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y%m%d')\n",
    "    if is_train:\n",
    "        data['transactionRevenue'] = data['transactionRevenue'].fillna(1).astype('float64')\n",
    "    data['hits'] = data['hits'].fillna(0).astype('int64')\n",
    "    data['newVisits'] = data['newVisits'].fillna(0).astype('int64')\n",
    "    data['pageviews'] = data['pageviews'].fillna(0).astype('int64')\n",
    "    data['page'] = data['page'].fillna(0).astype('int64')\n",
    "    data['bounces'] = data['bounces'].fillna(0).astype('int64')\n",
    "    data['isVideoAd'] = data['isVideoAd'].fillna('False').astype('bool')\n",
    "    data['isVideoAd'] = data['isVideoAd'].astype('int64')\n",
    "    data['visitStartTime'] = data['visitStartTime'].apply(lambda x: dt.datetime.utcfromtimestamp(float(x)))\n",
    "    data['visitHour'] = data['visitStartTime'].dt.hour\n",
    "    data['isMobile'] = data['isMobile'].astype('int64')\n",
    "#     data['fullVisitorId'] = data['fullVisitorId'].astype('float64')\n",
    "    \n",
    "    # dealing with nulls\n",
    "    for i in data.columns:\n",
    "        if data[i].isnull().sum() > 0:\n",
    "            data[i] = data[i].fillna('(not_set)')\n",
    "            \n",
    "    # dealing with string nulls      \n",
    "    data = data.replace(['not available in demo dataset', '(not set)'], ['(not_set)', '(not_set)'])\n",
    "    data['networkDomain'] = data['networkDomain'].replace(['unknown.unknown'], ['(not_set)'])\n",
    "    data['referralPath'] = data['referralPath'].replace(['/'], ['(not_set)'])\n",
    "    data['keyword'] = data['keyword'].replace(['(not provided)'], ['(not_set)'])\n",
    "    \n",
    "    if is_train:\n",
    "        data = data.assign(\n",
    "            logtransactionRevenue = np.log(data.transactionRevenue))\n",
    "        data['madePurchase'] = np.where(data['logtransactionRevenue'] > 0, 1, 0)\n",
    "    \n",
    "    print(data['fullVisitorId'].nunique(), ' 1')\n",
    "    # dealing with duplicate unique identifiers\n",
    "    _, data =  remove_session_id_dup(data, is_train)\n",
    "    print(data['fullVisitorId'].nunique(), ' 2')\n",
    "    if is_train:\n",
    "        data = data.drop(columns=['transactionRevenue', 'index', 'visitId', 'visitStartTime', 'gclId'])\n",
    "    else:\n",
    "        data = data.drop(columns=['index', 'visitId', 'visitStartTime', 'gclId'])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_session_id_dup(data, is_train):\n",
    "    sessionId_counts = pd.DataFrame(data['sessionId'].value_counts()).reset_index()\n",
    "    dup_sessionId = sessionId_counts['index'][sessionId_counts['sessionId']>1].values.tolist()\n",
    "    dup_data = data[data['sessionId'].isin(dup_sessionId)].sort_values(by='sessionId')\n",
    "    \n",
    "    index_to_delete = []\n",
    "    count = 0\n",
    "    delete = False\n",
    "    for index, row in dup_data.iterrows():\n",
    "        if is_train:\n",
    "            if count == 0 and row['madePurchase'] == 1: #leaves the first duplicate if the person has made a purchase\n",
    "                count +=1\n",
    "                delete = False\n",
    "                continue\n",
    "            elif count == 0 and delete == False: #deletes the first duplicate if person hasnt made a purchase\n",
    "                index_to_delete.append(index)\n",
    "                delete = True\n",
    "                count +=1\n",
    "            elif count == 1 and delete == False: #deletes the second duplicate if person has made a purchas in the first row\n",
    "                index_to_delete.append(index)\n",
    "                count = 0\n",
    "            elif count == 1 and delete == True: #assign delete to false and continues\n",
    "                delete = False\n",
    "                count = 0\n",
    "                continue\n",
    "        elif count == 0 and delete == False:\n",
    "            index_to_delete.append(index)\n",
    "            delete = True\n",
    "            count +=1\n",
    "        else:\n",
    "            delete = False\n",
    "            count = 0\n",
    "            \n",
    "    data = data.reset_index()\n",
    "    print('Deleted', len(index_to_delete), 'rows!')\n",
    "    print('Shape of data before deleting duplicates:', data.shape)\n",
    "    data_v1 = data[~data['index'].isin(index_to_delete)]\n",
    "    print('Shape of data after deleting duplicates:', data_v1.shape)   \n",
    "    \n",
    "    return index_to_delete, data_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903653, 12)\n",
      "714167  1\n",
      "Deleted 898 rows!\n",
      "Shape of data before deleting duplicates: (903653, 39)\n",
      "Shape of data after deleting duplicates: (902755, 39)\n",
      "714167  2\n",
      "(902755, 34)\n",
      "CPU times: user 1min 33s, sys: 6.48 s, total: 1min 39s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(train_data.shape)\n",
    "train_data_v1 = data_prep(train_data, is_train=True)\n",
    "print(train_data_v1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(804684, 12)\n",
      "617242  1\n",
      "Deleted 821 rows!\n",
      "Shape of data before deleting duplicates: (804684, 35)\n",
      "Shape of data after deleting duplicates: (803863, 35)\n",
      "617242  2\n",
      "(803863, 31)\n",
      "CPU times: user 1min 24s, sys: 4.8 s, total: 1min 29s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(test_data.shape)\n",
    "test_data_v1 = data_prep(test_data, is_train=False)\n",
    "print(test_data_v1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique customers in the train data are: 714167\n",
      "The number of unique customers in teh train data after apply data preparation techniques is: 714167\n",
      "The number of unique customers in the test data are: 617242\n",
      "The number of unique customers in the test data after apply data preparation techniques is: 617242\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique customers in the train data are:', train_data['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in teh train data after apply data preparation techniques is:', \n",
    "      train_data_v1['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the test data are:', test_data['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the test data after apply data preparation techniques is:', \n",
    "      test_data_v1['fullVisitorId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences(sentence):\n",
    "    #     nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    sentence = re.sub('[^a-zA-Z]+', ' ', sentence)\n",
    "    tokens = sentence.split()\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words] \n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens if len(token) > 1]\n",
    "    return ' '.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_sentences_v2(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    sentence = re.sub('[^a-zA-Z]+', ' ', sentence)\n",
    "    tokens = sentence.split('/')\n",
    "    filtered_sentence = [w for w in tokens if not w in stop_words] \n",
    "    stemmed_tokens = [porter_stemmer.stem(token.strip()) for token in tokens if len(token) > 1]\n",
    "    return ''.join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_columns(data, data_col_name, col_list, col_mark):\n",
    "    \n",
    "    for i in col_list:\n",
    "        col_name = col_mark + str(i)\n",
    "        data[col_name] = data[data_col_name].str.contains(i, case=1, na=0)\n",
    "        data[col_name] = data[col_name].astype(int)\n",
    "        \n",
    "    data = data.drop(columns=[data_col_name])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng(data):\n",
    "    # making features out of the date column\n",
    "    data['dayNameDate'] = data['date'].dt.day_name()\n",
    "    data['monthDate'] = data['date'].dt.month\n",
    "    data['yearDate'] = data['date'].dt.year\n",
    "    data['dayDate'] = data['date'].dt.day\n",
    "    \n",
    "    # make columns\n",
    "    network_domain_lst = ['(not_set)', '.us', '.net', '.com', '.edu', '.ca', '.org', '.mx' ]\n",
    "    operating_system_lst = ['Chrome OS', 'Macintosh', 'Linux', 'iOS', 'Windows']\n",
    "    ad_content_lst = ['Google Merchandise Collection', '(not_set)']\n",
    "    browser_lst = ['Chrome', 'Firefox', 'Internet Explorer', 'Edge']\n",
    "    country_lst = ['United States', 'Venezuela', 'Puerto Rico', 'Canada']\n",
    "    subContinent_lst = ['Northern America', 'Caribbean']\n",
    "    campaign_lst = ['AW - Dynamic Search Ads Whole Site', 'AW - Accessories', '(not_set)']\n",
    "    region_lst = ['Zulia', 'Nebraska', 'Michigan', 'Pichincha', 'Tennessee', 'Illinois',\n",
    "       'New York', 'Washington', 'Massachusetts', 'Colorado', 'South Carolina',\n",
    "       'Texas', 'Georgia', 'Missouri', 'Iowa', 'District of Columbia',\n",
    "       'California', 'Minnesota', 'Utah', 'Arizona', 'Pennsylvania',\n",
    "       'New Jersey', 'Indiana', 'Florida', 'Maryland', 'Connecticut',\n",
    "       'North Carolina', 'Virginia', 'Nevada', 'Ohio', 'Alberta', '(not_set)',\n",
    "       'Ontario', 'Oregon']\n",
    "    source_lst = ['mall.googleplex.com', 'dealspotr.com', 'mail.google.com',\n",
    "       'groups.google.com', 'phandroid.com', 'gdeals.googleplex.com', 'dfa',\n",
    "       'l.facebook.com', 'yahoo', 'google', 'bing', 'sites.google.com',\n",
    "       '(direct)', 'facebook.com']\n",
    "    city_lst = ['Maracaibo', 'Ann Arbor', 'Cambridge', 'San Bruno', 'Chicago',\n",
    "       'Austin', 'Irvine', 'New York', 'Nashville', 'Jersey City',\n",
    "       'Boulder', 'Kirkland', 'Seattle', 'Oakland', 'Denver', 'Sunnyvale',\n",
    "       'San Francisco', 'Pittsburgh', 'Washington', 'Atlanta',\n",
    "       'Los Angeles', 'Mountain View', 'Minneapolis', 'San Antonio',\n",
    "       'Lake Oswego', 'Santa Clara', 'Cupertino', 'Salem', 'San Mateo',\n",
    "       'San Diego', 'Palo Alto', 'Fremont', 'Houston', 'Milpitas',\n",
    "       'Boston', 'Charlotte', 'San Jose', 'Philadelphia', 'Redwood City',\n",
    "       'Portland', 'Phoenix', '(not_set)', 'Toronto', 'Dallas']\n",
    "    referalPath_lst = ['deal', 'sign', 'google', 'merchandise', 'store', 'emails',\n",
    "                       'special', 'coup', 'stor', 'mail', 'com', 'forum', 'merch', 'new',\n",
    "                       'url', 'site', 'mountain', 'view', 'php', 'offer', 'googletopia', \n",
    "                       'free', 'stuff', 'alphabet','discount']\n",
    "    keyword_lst = ['qehscssdk', 'googl', 'merchandis', 'store', 'hzbaqlcbjwfgoh', \n",
    "                       'remarket', 'content', 'target', 'zknv']\n",
    "    \n",
    "    data = make_columns(data, 'networkDomain', network_domain_lst, 'domain_')\n",
    "    data = make_columns(data, 'operatingSystem', operating_system_lst, 'os_')\n",
    "    data = make_columns(data, 'adContent', ad_content_lst, 'adContent_')\n",
    "    data = make_columns(data, 'browser', browser_lst, 'browser_')\n",
    "    data = make_columns(data, 'country', country_lst, 'country_')\n",
    "    data = make_columns(data, 'city', city_lst, 'city_')\n",
    "    data = make_columns(data, 'subContinent', subContinent_lst, 'subContinent_')\n",
    "    data = make_columns(data, 'campaign', campaign_lst, 'campaign_')\n",
    "    data = make_columns(data, 'region', region_lst, 'region_')\n",
    "    data = make_columns(data, 'source', source_lst, 'source_')\n",
    "    \n",
    "\n",
    "    data['clean_referralPath'] = data['referralPath'].apply(stem_sentences_v2)\n",
    "    data = make_columns(data, 'clean_referralPath', referalPath_lst, 'referralPath_')\n",
    "    \n",
    "    data['clean_keyword'] = data['keyword'].apply(stem_sentences)\n",
    "    data = make_columns(data, 'clean_keyword', keyword_lst, 'keyword_')\n",
    "    \n",
    "    data = data.drop(columns = ['date', 'metro', 'medium', 'referralPath', 'keyword'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 47s, sys: 1min 1s, total: 9min 49s\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_v2 = feature_eng(train_data_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique customers in the train data are: 714167\n",
      "The number of unique customers in the train data after apply data preparation techniques is: 714167\n",
      "The number of unique customers in the train data after apply feature engineering techniques is: 714167\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique customers in the train data are:', train_data['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the train data after apply data preparation techniques is:', \n",
    "      train_data_v1['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the train data after apply feature engineering techniques is:', \n",
    "      train_data_v2['fullVisitorId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 3s, sys: 1min, total: 9min 4s\n",
      "Wall time: 8min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_data_v2 = feature_eng(test_data_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique customers in the test data are: 617242\n",
      "The number of unique customers in the test data after apply data preparation techniques is: 617242\n",
      "The number of unique customers in the test data after apply data preparation techniques is: 617242\n"
     ]
    }
   ],
   "source": [
    "print('The number of unique customers in the test data are:', test_data['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the test data after apply data preparation techniques is:', \n",
    "      test_data_v1['fullVisitorId'].nunique())\n",
    "print('The number of unique customers in the test data after apply data preparation techniques is:', \n",
    "      test_data_v2['fullVisitorId'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_v2.to_csv('~/customer_revenue_prediction/data/cleaned_feat_eng_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v2.to_csv('~/customer_revenue_prediction/data/cleaned_feat_eng_train_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n",
    "\n",
    "### Idea 1\n",
    "    - Run logistic regression for classification with a select few features\n",
    "    - Run logistic regression to predict logtransactionrevenue for the rows that were classified as 1\n",
    "\n",
    "### Logistic Regression Evaluation Metrics for Classification Model\n",
    "    - confusion matrix\n",
    "    - ROC curve\n",
    "    - accuracy\n",
    "    \n",
    "Since this is the first iteration, no kfold\n",
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_v2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data_v2['madePurchase']\n",
    "train_data_v3 = train_data_v2.drop(columns = ['sessionId', 'logtransactionRevenue', 'madePurchase', 'keyword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_v2 = pd.get_dummies(train_v1)\n",
    "train_data_model = pd.get_dummies(train_data_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_model.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(train_data_model, y, test_size=0.2, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_validate)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_validate, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_validate, logreg.predict(X_validate))\n",
    "fpr, tpr, thresholds = roc_curve(y_validate, logreg.predict_proba(X_validate)[:,1])\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try SMOTE to Even Out Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample a 100000 rows\n",
    "subset_train_data_v3 = train_data_v3.sample(100000, random_state=0)\n",
    "subset_y = y.sample(100000, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = subset_train_data_v5[['channelGrouping', 'visitNumber', 'deviceCategory', 'isMobile',\n",
    "#                           'continent', 'bounces', 'hits','newVisits', 'pageviews', 'adContent', 'isVideoAd',\n",
    "#                           'page', 'slot', 'visitHour', 'dayNameDate', 'monthDate']]\n",
    "X_v1 = pd.get_dummies(subset_train_data_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os = SMOTE(random_state=0)\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_v1, subset_y, test_size=0.2, random_state=0)\n",
    "columns = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train)\n",
    "os_data_X = pd.DataFrame(data=os_data_X,columns=columns )\n",
    "os_data_y= pd.DataFrame(data=os_data_y,columns=['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This is the original number of rows of the subset sample:', subset_y.shape[0])\n",
    "print('This is the new number of rows of the subset sample:', os_data_y.shape[0])\n",
    "print('Number of rows that have made a purchase increased from',\n",
    "      subset_y[subset_y==1].shape[0],'to',os_data_y[os_data_y.y == 1].shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 50)\n",
    "rfe = rfe.fit(os_data_X, os_data_y.values.ravel())\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_bool = rfe.support_\n",
    "rfe_cols = []\n",
    "for i in enumerate(os_data_X.columns):\n",
    "    if col_bool[i[0]] == True:\n",
    "        rfe_cols.append(i[1])\n",
    "os_data_X_rfe = os_data_X[rfe_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X_rfe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X_rfe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "logit_model=sm.Logit(os_data_y,os_data_X_rfe)\n",
    "result=logit_model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(os_data_X_rfe, os_data_y, test_size=0.2, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_validate)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_validate, y_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_validate, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logit_roc_auc = roc_auc_score(y_validate, logreg.predict(X_validate))\n",
    "fpr, tpr, thresholds = roc_curve(y_validate, logreg.predict_proba(X_validate)[:,1])\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os_data_X_rfe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply PCA on RFE Data and Plot to Identify Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_os_data_X_rfe = scaler.fit_transform(os_data_X_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_os_data_X_rfe = pd.DataFrame(scaled_os_data_X_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_scaled_os_data_X_rfe = pca.fit_transform(scaled_os_data_X_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_scaled_os_data_X_rfe = pd.DataFrame(pca_scaled_os_data_X_rfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot_data = pd.concat([pca_scaled_os_data_X_rfe,os_data_y ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot_data.columns = ['x1', 'x2', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('x1', 'x2', data=sns_plot_data, hue='y', scatter_kws={'alpha':0.3}, fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot('x1', 'x2', data=sns_plot_data, scatter_kws={'alpha':0.3}, fit_reg=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see from the data that there is a difference between 1 and 0 for made purchases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold for feature eng method for certain columns\n",
    "    - max percentage of dataset\n",
    "    - column name\n",
    "    - list of words to make columns\n",
    "    - stemming of columns\n",
    "        - if stemming, what to split the word by?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For week of Sep 24th\n",
    "    - See how the logistic method implemented above performs when certain rows for the browsers are deleted. If it performs the same or better move that into the feature engineering method because data is not too much of an issue\n",
    "    - Run logistic regression models on sampled data with various RFE components to see which is the best starting with all\n",
    "\n",
    "- build a simple model\n",
    "    - Build a model to classify data as made transaction or did not make transaction\n",
    "        - optimize that model and identify if possible to move on to next stage and predict transaction revenue\n",
    "    - Build model to sum transaction for the specific row using the training data of ones with only transactions\n",
    "        - identify if this is a better approach then using all models\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models that I will test out:\n",
    "    - Logistic Regression\n",
    "    - Random Forrest\n",
    "    - Boosting\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
